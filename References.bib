@Article{Smith2020,
AUTHOR = { Smith, RA and Schneider, PP},
TITLE = {Making health economic models Shiny: A tutorial [version 2; peer review: 2 approved]
},
JOURNAL = {Wellcome Open Research},
VOLUME = {5},
YEAR = {2020},
NUMBER = {69},
DOI = {10.12688/wellcomeopenres.15807.2}
}
@Article{Strong2012,
author = {Strong, Mark and Oakley, Jeremy E. and Chilcott, Jim},
title = {Managing structural uncertainty in health economic decision models: a discrepancy approach},
journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
volume = {61},
number = {1},
pages = {25-45},
keywords = {Computer model, Elicitation, Health economics, Model uncertainty, Sensitivity analysis, Uncertainty analysis},
doi = {https://doi.org/10.1111/j.1467-9876.2011.01014.x},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9876.2011.01014.x},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9876.2011.01014.x},
abstract = {Summary. Healthcare resource allocation decisions are commonly informed by computer model predictions of population mean costs and health effects. It is common to quantify the uncertainty in the prediction due to uncertain model inputs, but methods for quantifying uncertainty due to inadequacies in model structure are less well developed. We introduce an example of a model that aims to predict the costs and health effects of a physical activity promoting intervention. Our goal is to develop a framework in which we can manage our uncertainty about the costs and health effects due to deficiencies in the model structure. We describe the concept of ‘model discrepancy’: the difference between the model evaluated at its true inputs, and the true costs and health effects. We then propose a method for quantifying discrepancy based on decomposing the cost-effectiveness model into a series of subfunctions, and considering potential error at each subfunction. We use a variance-based sensitivity analysis to locate important sources of discrepancy within the model to guide model refinement. The resulting improved model is judged to contain less structural error, and the distribution on the model output better reflects our true uncertainty about the costs and effects of the intervention.},
year = {2012}
}
@Article{Enns2015,
author = {Eva A. Enns and Lauren E. Cipriano and Cyrena T. Simons and Chung Yin Kong},
title ={Identifying Best-Fitting Inputs in Health-Economic Model Calibration: A Pareto Frontier Approach},

journal = {Medical Decision Making},
volume = {35},
number = {2},
pages = {170-182},
year = {2015},
doi = {10.1177/0272989X14528382},
    note ={PMID: 24799456},

URL = { 
    
        https://doi.org/10.1177/0272989X14528382
},
eprint = { 
        https://doi.org/10.1177/0272989X14528382
}
,
    abstract = { Background. To identify best-fitting input sets using model calibration, individual calibration target fits are often combined into a single goodness-of-fit (GOF) measure using a set of weights. Decisions in the calibration process, such as which weights to use, influence which sets of model inputs are identified as best-fitting, potentially leading to different health economic conclusions. We present an alternative approach to identifying best-fitting input sets based on the concept of Pareto-optimality. A set of model inputs is on the Pareto frontier if no other input set simultaneously fits all calibration targets as well or better. Methods. We demonstrate the Pareto frontier approach in the calibration of 2 models: a simple, illustrative Markov model and a previously published cost-effectiveness model of transcatheter aortic valve replacement (TAVR). For each model, we compare the input sets on the Pareto frontier to an equal number of best-fitting input sets according to 2 possible weighted-sum GOF scoring systems, and we compare the health economic conclusions arising from these different definitions of best-fitting. Results. For the simple model, outcomes evaluated over the best-fitting input sets according to the 2 weighted-sum GOF schemes were virtually nonoverlapping on the cost-effectiveness plane and resulted in very different incremental cost-effectiveness ratios (\$79,300 [95\% CI 72,500–87,600] v. \$139,700 [95\% CI 79,900–182,800] per quality-adjusted life-year [QALY] gained). Input sets on the Pareto frontier spanned both regions (\$79,000 [95\% CI 64,900–156,200] per QALY gained). The TAVR model yielded similar results. Conclusions. Choices in generating a summary GOF score may result in different health economic conclusions. The Pareto frontier approach eliminates the need to make these choices by using an intuitive and transparent notion of optimality as the basis for identifying best-fitting input sets. }
}
@inprocedings{Bryning2022,
   author = {Bryning, S. and Grimsey Jones, F. and Pollit, V. and Brennan, A.},
   title = {EE81 When Is a Simple Model Good Enough? Towards Greater Use of Formal Methods for Evaluating Structural Uncertainty in Cost-Effectiveness Analysis},
   journal = {Value in Health},
   volume = {25},
   number = {7},
   pages = {S350-S351},
   DOI = {10.1016/j.jval.2022.04.335},
   year = {2022},
}

@article{Strong2015,
author = {Mark Strong and Jeremy E. Oakley and Alan Brennan and Penny Breeze},
title ={Estimating the Expected Value of Sample Information Using the Probabilistic Sensitivity Analysis Sample: A Fast, Nonparametric Regression-Based Method},

journal = {Medical Decision Making},
volume = {35},
number = {5},
pages = {570-583},
year = {2015},
doi = {10.1177/0272989X15575286},
    note ={PMID: 25810269},

URL = { 
    
        https://doi.org/10.1177/0272989X15575286
},
eprint = { 
    
        https://doi.org/10.1177/0272989X15575286
}
 }

